{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning on Flickr8k Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xXcp0V7cabWX"
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from pickle import dump\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from numpy import argmax\n",
    "import tensorflow as tf\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SMaqmyQAh7IS",
    "outputId": "e850fabc-cdfb-4931-b841-cb6b868c7283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Jrt1un6mh9_z",
    "outputId": "00dd7c0b-b6eb-4a0d-df39-ab7f10846ae0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "fG91S115iAB_",
    "outputId": "ab98757d-cd4a-4120-9196-0306e264044c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vcG8P5cUkvol",
    "outputId": "914d0af7-87ee-41d3-f8d8-d08e9ac2d0b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      ">17273391_55cfc7d3d4.jpg\n",
      ">23445819_3a458716c1.jpg\n",
      ">35506150_cbdb630f4f.jpg\n",
      ">19212715_20476497a3.jpg\n",
      ">667626_18933d713e.jpg\n",
      ">3637013_c675de7705.jpg\n",
      ">33108590_d685bfe51c.jpg\n",
      ">27782020_4dab210360.jpg\n",
      ">10815824_2997e03d76.jpg\n",
      ">12830823_87d2654e31.jpg\n",
      "Extracted Features : 10\n"
     ]
    }
   ],
   "source": [
    "def extract_features(directory):\n",
    "  # load the model\n",
    "  model = VGG16()\n",
    "  # re-structure the model\n",
    "  model.layers.pop()\n",
    "  model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "  # summarize\n",
    "  print(model.summary())\n",
    "  # extract features from each photo\n",
    "  features = dict()\n",
    "  for name in listdir(directory):\n",
    "    # load an image from file\n",
    "    filename = directory + '/' + name\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    # convert the image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "    # reshape data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # prepare the image for the VGG model\n",
    "    image = preprocess_input(image)\n",
    "    # get features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    # get image id\n",
    "    image_id = name.split('.')[0]\n",
    "    # store feature\n",
    "    features[image_id] = feature\n",
    "    print('>%s' % name)\n",
    "  return features\n",
    "\n",
    " \n",
    "# extract features from all images\n",
    "directory = 'drive/My Drive/IC/Flicker8k_Dataset_Small'\n",
    "features = extract_features(directory)\n",
    "print('Extracted Features : %d' % len(features))\n",
    "# save to file\n",
    "dump(features, open('drive/My Drive/IC/features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the file with image-name and associated captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "colab_type": "code",
    "id": "Tt6lmSN2uTXO",
    "outputId": "945c657d-1576-41e2-e648-559461ad2f81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "667626_18933d713e.jpg#0\tA girl is stretched out in shallow water\n",
      "667626_18933d713e.jpg#1\tA girl wearing a red and multi-colored bikini is laying on her back in shallow water .\n",
      "667626_18933d713e.jpg#2\tA little girl in a red swimsuit is laying on her back in shallow water .\n",
      "667626_18933d713e.jpg#3\tA young girl is lying in the sand , while ocean water is surrounding her .\n",
      "667626_18933d713e.jpg#4\tGirl wearing a bikini lying on her back in a shallow pool of clear blue water .\n",
      "3637013_c675de7705.jpg#0\tA couple stands close at the water 's edge .\n",
      "3637013_c675de7705.jpg#1\tThe two people stand by a body of water and in front of bushes in fall .\n",
      "3637013_c675de7705.jpg#2\tTwo people hold each other near a pond .\n",
      "3637013_c675de7705.jpg#3\tTwo people stand by the water .\n",
      "3637013_c675de7705.jpg#4\tTwo people stand together on the edge of the water on the grass .\n",
      "10815824_2997e03d76.jpg#0\tA blonde horse and a blonde girl in a black sweatshirt are staring at a fire in a barrel .\n",
      "10815824_2997e03d76.jpg#1\tA girl and her horse stand by a fire .\n",
      "10815824_2997e03d76.jpg#2\tA girl holding a horse 's lead behind a fire .\n",
      "10815824_2997e03d76.jpg#3\tA man , and girl and two horses are near a contained fire .\n",
      "10815824_2997e03d76.jpg#4\tTwo people and two horses watching a fire .\n",
      "12830823_87d2654e31.jpg#0\tChildren sit and watch the fish moving in the pond\n",
      "12830823_87d2654e31.jpg#1\tpeople stare at the orange fish .\n",
      "12830823_87d2654e31.jpg#2\tSeveral people are standing near a fish pond .\n",
      "12830823_87d2654e31.jpg#3\tSome children watching fish in a pool .\n",
      "12830823_87d2654e31.jpg#4\tThere are several people and children looking into water with a blue tiled floor and goldfish .\n",
      "17273391_55cfc7d3d4.jpg#0\tA fisherman fishes at the bank of a foggy river .\n",
      "17273391_55cfc7d3d4.jpg#1\tA man fishes by a tree in the morning mist .\n",
      "17273391_55cfc7d3d4.jpg#2\tA man fishes under a large tree .\n",
      "17273391_55cfc7d3d4.jpg#3\tA man fishing near a large tree .\n",
      "17273391_55cfc7d3d4.jpg#4\tA man is fishing in a foggy lake .\n",
      "19212715_20476497a3.jpg#0\ta kayaker kayaks through the water .\n",
      "19212715_20476497a3.jpg#1\tA person kayaking in the ocean .\n",
      "19212715_20476497a3.jpg#2\tA person kayaks in the middle of the ocean on a grey day .\n",
      "19212715_20476497a3.jpg#3\tA person rows a boat over a large body of water .\n",
      "19212715_20476497a3.jpg#4\tperson in a boat with a paddle in hand\n",
      "23445819_3a458716c1.jpg#0\ta beagle and a golden retriever wrestling in the grass\n",
      "23445819_3a458716c1.jpg#1\tTwo dogs are wrestling in the grass .\n",
      "23445819_3a458716c1.jpg#2\tTwo puppies are playing in the green grass .\n",
      "23445819_3a458716c1.jpg#3\ttwo puppies playing around in the grass\n",
      "23445819_3a458716c1.jpg#4\tTwo puppies play in the grass\n",
      "27782020_4dab210360.jpg#0\ta brightly decorated bicycle with cart with people walking around in the background\n",
      "27782020_4dab210360.jpg#1\tA street vending machine is parked while people walk by .\n",
      "27782020_4dab210360.jpg#2\tA street vendor on the corner of a busy intersection .\n",
      "27782020_4dab210360.jpg#3\tPeople on the city street walk past a puppet theater .\n",
      "27782020_4dab210360.jpg#4\tPeople walk around a mobile puppet theater in a big city .\n",
      "33108590_d685bfe51c.jpg#0\tA young couple inspect merchandise from a street vendor .\n",
      "33108590_d685bfe51c.jpg#1\tPeople are gathering around a table of food and outside a taxi wisks by .\n",
      "33108590_d685bfe51c.jpg#2\tThree people prepare a table full of food with a police car in the background .\n",
      "33108590_d685bfe51c.jpg#3\ttwo people look at a street vendor .\n",
      "33108590_d685bfe51c.jpg#4\tTwo women and a man at a food counter in dim lights .\n",
      "35506150_cbdb630f4f.jpg#0\tA man in a red jacket is sitting on a bench whilst cooking a meal .\n",
      "35506150_cbdb630f4f.jpg#1\tA man is sitting on a bench , cooking some food .\n",
      "35506150_cbdb630f4f.jpg#2\tA man sits on a bench .\n",
      "35506150_cbdb630f4f.jpg#3\tA man wearing a red jacket is sitting on a wooden bench and is cooking something in a small pot .\n",
      "35506150_cbdb630f4f.jpg#4\ta man wearing a red jacket sitting on a bench next to various camping items\n"
     ]
    }
   ],
   "source": [
    "# Load data from text file\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "filename = 'drive/My Drive/IC/Flicker8k_token_Small.txt'\n",
    "# load descriptions\n",
    "doc = load_doc(filename)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping the image-name and five associated captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "SG7yAjhPP8k2",
    "outputId": "e58d67df-2e79-467b-ac5e-a031ba6b5b35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'667626_18933d713e': ['A girl is stretched out in shallow water', 'A girl wearing a red and multi-colored bikini is laying on her back in shallow water .', 'A little girl in a red swimsuit is laying on her back in shallow water .', 'A young girl is lying in the sand , while ocean water is surrounding her .', 'Girl wearing a bikini lying on her back in a shallow pool of clear blue water .'], '3637013_c675de7705': [\"A couple stands close at the water 's edge .\", 'The two people stand by a body of water and in front of bushes in fall .', 'Two people hold each other near a pond .', 'Two people stand by the water .', 'Two people stand together on the edge of the water on the grass .'], '10815824_2997e03d76': ['A blonde horse and a blonde girl in a black sweatshirt are staring at a fire in a barrel .', 'A girl and her horse stand by a fire .', \"A girl holding a horse 's lead behind a fire .\", 'A man , and girl and two horses are near a contained fire .', 'Two people and two horses watching a fire .'], '12830823_87d2654e31': ['Children sit and watch the fish moving in the pond', 'people stare at the orange fish .', 'Several people are standing near a fish pond .', 'Some children watching fish in a pool .', 'There are several people and children looking into water with a blue tiled floor and goldfish .'], '17273391_55cfc7d3d4': ['A fisherman fishes at the bank of a foggy river .', 'A man fishes by a tree in the morning mist .', 'A man fishes under a large tree .', 'A man fishing near a large tree .', 'A man is fishing in a foggy lake .'], '19212715_20476497a3': ['a kayaker kayaks through the water .', 'A person kayaking in the ocean .', 'A person kayaks in the middle of the ocean on a grey day .', 'A person rows a boat over a large body of water .', 'person in a boat with a paddle in hand'], '23445819_3a458716c1': ['a beagle and a golden retriever wrestling in the grass', 'Two dogs are wrestling in the grass .', 'Two puppies are playing in the green grass .', 'two puppies playing around in the grass', 'Two puppies play in the grass'], '27782020_4dab210360': ['a brightly decorated bicycle with cart with people walking around in the background', 'A street vending machine is parked while people walk by .', 'A street vendor on the corner of a busy intersection .', 'People on the city street walk past a puppet theater .', 'People walk around a mobile puppet theater in a big city .'], '33108590_d685bfe51c': ['A young couple inspect merchandise from a street vendor .', 'People are gathering around a table of food and outside a taxi wisks by .', 'Three people prepare a table full of food with a police car in the background .', 'two people look at a street vendor .', 'Two women and a man at a food counter in dim lights .'], '35506150_cbdb630f4f': ['A man in a red jacket is sitting on a bench whilst cooking a meal .', 'A man is sitting on a bench , cooking some food .', 'A man sits on a bench .', 'A man wearing a red jacket is sitting on a wooden bench and is cooking something in a small pot .', 'a man wearing a red jacket sitting on a bench next to various camping items']}\n",
      "Loaded: 10 \n"
     ]
    }
   ],
   "source": [
    "# extract descriptions for images\n",
    "def load_descriptions(doc):\n",
    "  mapping = dict()\n",
    "  # process lines\n",
    "  for line in doc.split('\\n'):\n",
    "    # split line by white space\n",
    "    tokens = line.split()\n",
    "    if len(line) < 2:\n",
    "      continue\n",
    "    # take the first token as the image id, the rest as the description\n",
    "    image_id, image_desc = tokens[0], tokens[1:]\n",
    "    # remove filename from image id\n",
    "    image_id = image_id.split('.')[0]\n",
    "    # convert description tokens back to string\n",
    "    image_desc = ' '.join(image_desc)\n",
    "    # create the list if needed\n",
    "    if image_id not in mapping:\n",
    "      mapping[image_id] = list()\n",
    "    # store description\n",
    "    mapping[image_id].append(image_desc)\n",
    "  print(mapping)\n",
    "  return mapping\n",
    "\n",
    "# parse descriptions\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing punctuations, numbers and coverting to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "Cf2BgI1oTIwu",
    "outputId": "85426cbe-8ce6-48bd-e752-cbe8bd061175"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'667626_18933d713e': ['girl is stretched out in shallow water', 'girl wearing red and multicolored bikini is laying on her back in shallow water', 'little girl in red swimsuit is laying on her back in shallow water', 'young girl is lying in the sand while ocean water is surrounding her', 'girl wearing bikini lying on her back in shallow pool of clear blue water'], '3637013_c675de7705': ['couple stands close at the water edge', 'the two people stand by body of water and in front of bushes in fall', 'two people hold each other near pond', 'two people stand by the water', 'two people stand together on the edge of the water on the grass'], '10815824_2997e03d76': ['blonde horse and blonde girl in black sweatshirt are staring at fire in barrel', 'girl and her horse stand by fire', 'girl holding horse lead behind fire', 'man and girl and two horses are near contained fire', 'two people and two horses watching fire'], '12830823_87d2654e31': ['children sit and watch the fish moving in the pond', 'people stare at the orange fish', 'several people are standing near fish pond', 'some children watching fish in pool', 'there are several people and children looking into water with blue tiled floor and goldfish'], '17273391_55cfc7d3d4': ['fisherman fishes at the bank of foggy river', 'man fishes by tree in the morning mist', 'man fishes under large tree', 'man fishing near large tree', 'man is fishing in foggy lake'], '19212715_20476497a3': ['kayaker kayaks through the water', 'person kayaking in the ocean', 'person kayaks in the middle of the ocean on grey day', 'person rows boat over large body of water', 'person in boat with paddle in hand'], '23445819_3a458716c1': ['beagle and golden retriever wrestling in the grass', 'two dogs are wrestling in the grass', 'two puppies are playing in the green grass', 'two puppies playing around in the grass', 'two puppies play in the grass'], '27782020_4dab210360': ['brightly decorated bicycle with cart with people walking around in the background', 'street vending machine is parked while people walk by', 'street vendor on the corner of busy intersection', 'people on the city street walk past puppet theater', 'people walk around mobile puppet theater in big city'], '33108590_d685bfe51c': ['young couple inspect merchandise from street vendor', 'people are gathering around table of food and outside taxi wisks by', 'three people prepare table full of food with police car in the background', 'two people look at street vendor', 'two women and man at food counter in dim lights'], '35506150_cbdb630f4f': ['man in red jacket is sitting on bench whilst cooking meal', 'man is sitting on bench cooking some food', 'man sits on bench', 'man wearing red jacket is sitting on wooden bench and is cooking something in small pot', 'man wearing red jacket sitting on bench next to various camping items']}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    " \n",
    "def clean_descriptions(descriptions):\n",
    "  # prepare translation table for removing punctuation\n",
    "  table = str.maketrans('', '', string.punctuation)\n",
    "  for key, desc_list in descriptions.items():\n",
    "    for i in range(len(desc_list)):\n",
    "      desc = desc_list[i]\n",
    "      # tokenize\n",
    "      desc = desc.split()\n",
    "      # convert to lower case\n",
    "      desc = [word.lower() for word in desc]\n",
    "      # remove punctuation from each token\n",
    "      desc = [w.translate(table) for w in desc]\n",
    "      # remove hanging 's' and 'a'\n",
    "      desc = [word for word in desc if len(word)>1]\n",
    "      # remove tokens with numbers in them\n",
    "      desc = [word for word in desc if word.isalpha()]\n",
    "      # store as string\n",
    "      desc_list[i] =  ' '.join(desc)\n",
    " \n",
    "# clean descriptions\n",
    "clean_descriptions(descriptions)\n",
    "print(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "fccc3_foWEpF",
    "outputId": "07a2a4ad-9d15-481f-8006-1c4b3124a28e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'close', 'little', 'black', 'camping', 'kayaking', 'red', 'blonde', 'look', 'under', 'decorated', 'pond', 'orange', 'three', 'out', 'watch', 'bench', 'hand', 'people', 'standing', 'barrel', 'goldfish', 'counter', 'while', 'shallow', 'sit', 'green', 'kayaks', 'lying', 'several', 'couple', 'sweatshirt', 'theater', 'edge', 'walking', 'man', 'day', 'vending', 'through', 'around', 'sits', 'contained', 'wrestling', 'children', 'and', 'ocean', 'play', 'small', 'pot', 'girl', 'river', 'stare', 'fisherman', 'hold', 'there', 'watching', 'puppies', 'middle', 'intersection', 'tiled', 'surrounding', 'kayaker', 'young', 'are', 'on', 'rows', 'items', 'holding', 'merchandise', 'front', 'looking', 'city', 'lights', 'horse', 'dogs', 'bicycle', 'background', 'near', 'jacket', 'morning', 'her', 'laying', 'beagle', 'in', 'by', 'grass', 'person', 'meal', 'retriever', 'next', 'sand', 'large', 'whilst', 'at', 'busy', 'big', 'over', 'sitting', 'boat', 'bank', 'bikini', 'lake', 'together', 'women', 'stand', 'playing', 'into', 'fishes', 'dim', 'car', 'corner', 'two', 'each', 'staring', 'cart', 'is', 'of', 'blue', 'fire', 'with', 'machine', 'various', 'wisks', 'brightly', 'to', 'multicolored', 'pool', 'parked', 'bushes', 'moving', 'fish', 'vendor', 'table', 'golden', 'taxi', 'from', 'something', 'lead', 'outside', 'the', 'gathering', 'prepare', 'stretched', 'fishing', 'body', 'walk', 'water', 'paddle', 'police', 'swimsuit', 'wooden', 'back', 'street', 'horses', 'wearing', 'food', 'cooking', 'stands', 'past', 'clear', 'grey', 'mist', 'behind', 'some', 'tree', 'mobile', 'fall', 'puppet', 'other', 'foggy', 'full', 'floor', 'inspect'}\n",
      "Vocabulary Size: 172\n"
     ]
    }
   ],
   "source": [
    "# convert the loaded descriptions into a vocabulary of words\n",
    "def to_vocabulary(descriptions):\n",
    "  # build a list of all description strings\n",
    "  all_desc = set()\n",
    "  for key in descriptions.keys():\n",
    "    [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "  return all_desc\n",
    " \n",
    "# summarize vocabulary\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print(vocabulary)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YnZFDXnXTYD"
   },
   "outputs": [],
   "source": [
    "# save descriptions to file, one per line\n",
    "def save_descriptions(descriptions, filename):\n",
    "  lines = list()\n",
    "  for key, desc_list in descriptions.items():\n",
    "    for desc in desc_list:\n",
    "      lines.append(key + ' ' + desc)\n",
    "  data = '\\n'.join(lines)\n",
    "  file = open(filename, 'w')\n",
    "  file.write(data)\n",
    "  file.close()\n",
    "\n",
    "# save descriptions\n",
    "save_descriptions(descriptions, 'drive/My Drive/IC/descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "p1Y7fJl5XRfl",
    "outputId": "c8d41a0a-4fe9-4c3c-ed11-0a6dfcdd1e46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 10\n",
      "Descriptions: train=10\n",
      "Photos: 10\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "  # open the file as read only\n",
    "  file = open(filename, 'r')\n",
    "  # read all text\n",
    "  text = file.read()\n",
    "  # close the file\n",
    "  file.close()\n",
    "  return text\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "  doc = load_doc(filename)\n",
    "  dataset = list()\n",
    "  # process line by line\n",
    "  for line in doc.split('\\n'):\n",
    "    # skip empty lines\n",
    "    if len(line) < 1:\n",
    "      continue\n",
    "    # get the image identifier\n",
    "    identifier = line.split('.')[0]\n",
    "    dataset.append(identifier)\n",
    "  return set(dataset)\n",
    "\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "  # load document\n",
    "  doc = load_doc(filename)\n",
    "  descriptions = dict()\n",
    "  for line in doc.split('\\n'):\n",
    "    # split line by white space\n",
    "    tokens = line.split()\n",
    "    # split id from description\n",
    "    image_id, image_desc = tokens[0], tokens[1:]\n",
    "    # skip images not in the set\n",
    "    if image_id in dataset:\n",
    "      # create list\n",
    "      if image_id not in descriptions:\n",
    "        descriptions[image_id] = list()\n",
    "      # wrap description in tokens\n",
    "      desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "      # store\n",
    "      descriptions[image_id].append(desc)\n",
    "  return descriptions\n",
    "\n",
    "# load photo features (loading from pickle data)\n",
    "def load_photo_features(filename, dataset):\n",
    "  # load all features\n",
    "  all_features = load(open(filename, 'rb'))\n",
    "  # filter features\n",
    "  features = {k: all_features[k] for k in dataset}\n",
    "  return features\n",
    "\n",
    "# load training dataset\n",
    "filename = 'drive/My Drive/IC/Flickr8k_trainImages_Small.txt'\n",
    "train = load_set(filename) # Training images name in \"set\" (dataset)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('drive/My Drive/IC/descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('drive/My Drive/IC/features.pkl', train)\n",
    "print('Photos: %d' % len(train_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping of word to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "gD8IS887mdAQ",
    "outputId": "fde75d4d-5e79-4309-a2fc-910e6d450ae7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'startseq': 1, 'endseq': 2, 'in': 3, 'the': 4, 'people': 5, 'and': 6, 'on': 7, 'two': 8, 'water': 9, 'is': 10, 'man': 11, 'of': 12, 'girl': 13, 'are': 14, 'at': 15, 'by': 16, 'grass': 17, 'red': 18, 'her': 19, 'fire': 20, 'with': 21, 'street': 22, 'bench': 23, 'shallow': 24, 'wearing': 25, 'stand': 26, 'near': 27, 'fish': 28, 'person': 29, 'around': 30, 'food': 31, 'sitting': 32, 'back': 33, 'ocean': 34, 'pond': 35, 'horse': 36, 'children': 37, 'fishes': 38, 'tree': 39, 'large': 40, 'puppies': 41, 'walk': 42, 'vendor': 43, 'jacket': 44, 'cooking': 45, 'bikini': 46, 'laying': 47, 'young': 48, 'lying': 49, 'while': 50, 'pool': 51, 'blue': 52, 'couple': 53, 'edge': 54, 'body': 55, 'blonde': 56, 'horses': 57, 'watching': 58, 'several': 59, 'some': 60, 'foggy': 61, 'fishing': 62, 'kayaks': 63, 'boat': 64, 'wrestling': 65, 'playing': 66, 'background': 67, 'city': 68, 'puppet': 69, 'theater': 70, 'table': 71, 'stretched': 72, 'out': 73, 'multicolored': 74, 'little': 75, 'swimsuit': 76, 'sand': 77, 'surrounding': 78, 'clear': 79, 'stands': 80, 'close': 81, 'front': 82, 'bushes': 83, 'fall': 84, 'hold': 85, 'each': 86, 'other': 87, 'together': 88, 'black': 89, 'sweatshirt': 90, 'staring': 91, 'barrel': 92, 'holding': 93, 'lead': 94, 'behind': 95, 'contained': 96, 'sit': 97, 'watch': 98, 'moving': 99, 'stare': 100, 'orange': 101, 'standing': 102, 'there': 103, 'looking': 104, 'into': 105, 'tiled': 106, 'floor': 107, 'goldfish': 108, 'fisherman': 109, 'bank': 110, 'river': 111, 'morning': 112, 'mist': 113, 'under': 114, 'lake': 115, 'kayaker': 116, 'through': 117, 'kayaking': 118, 'middle': 119, 'grey': 120, 'day': 121, 'rows': 122, 'over': 123, 'paddle': 124, 'hand': 125, 'beagle': 126, 'golden': 127, 'retriever': 128, 'dogs': 129, 'green': 130, 'play': 131, 'brightly': 132, 'decorated': 133, 'bicycle': 134, 'cart': 135, 'walking': 136, 'vending': 137, 'machine': 138, 'parked': 139, 'corner': 140, 'busy': 141, 'intersection': 142, 'past': 143, 'mobile': 144, 'big': 145, 'inspect': 146, 'merchandise': 147, 'from': 148, 'gathering': 149, 'outside': 150, 'taxi': 151, 'wisks': 152, 'three': 153, 'prepare': 154, 'full': 155, 'police': 156, 'car': 157, 'look': 158, 'women': 159, 'counter': 160, 'dim': 161, 'lights': 162, 'whilst': 163, 'meal': 164, 'sits': 165, 'wooden': 166, 'something': 167, 'small': 168, 'pot': 169, 'next': 170, 'to': 171, 'various': 172, 'camping': 173, 'items': 174}\n",
      "Vocabulary Size: 175\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# convert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "  all_desc = list()\n",
    "  for key in descriptions.keys():\n",
    "    [all_desc.append(d) for d in descriptions[key]]\n",
    "  return all_desc\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "  lines = to_lines(descriptions)\n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(lines)\n",
    "  return tokenizer\n",
    "\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "print(tokenizer.word_index)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Z1CwKIggpbEC",
    "outputId": "64b7e2fa-ae33-4b50-a0b4-384bd905680c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of caption in dataset :  18\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import load\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
    "  X1, X2, y = list(), list(), list()\n",
    "  # walk through each image identifier\n",
    "  for key, desc_list in descriptions.items():\n",
    "    # walk through each description for the image\n",
    "    for desc in desc_list:\n",
    "      # encode the sequence\n",
    "      seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "      # split one sequence into multiple X,y pairs\n",
    "      for i in range(1, len(seq)):\n",
    "        # split into input and output pair\n",
    "        in_seq, out_seq = seq[:i], seq[i]\n",
    "        # pad input sequence\n",
    "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "        # encode output sequence\n",
    "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "        # store\n",
    "        X1.append(photos[key][0])\n",
    "        X2.append(in_seq)\n",
    "        y.append(out_seq)\n",
    "  return array(X1), array(X2), array(y)\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)\n",
    "\n",
    "max_length = max_length(train_descriptions)\n",
    "print(\"Maximum length of caption in dataset : \", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1FoU4qByp7p3"
   },
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "  # feature extractor model\n",
    "  inputs1 = Input(shape=(4096,))\n",
    "  fe1 = Dropout(0.5)(inputs1)\n",
    "  fe2 = Dense(256, activation='relu')(fe1)\n",
    "  # sequence model\n",
    "  inputs2 = Input(shape=(max_length,))\n",
    "  se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "  se2 = Dropout(0.5)(se1)\n",
    "  se3 = LSTM(256)(se2)\n",
    "  # decoder model\n",
    "  decoder1 = add([fe2, se3])\n",
    "  decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "  outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "  # tie it together [image, seq] [word]\n",
    "  model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "  # summarize model\n",
    "  print(model.summary())\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZyrnsMJBzVlg",
    "outputId": "67f71c6b-1a15-4a52-9fce-cecd6f806d67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 Shape : (497, 4096)\n",
      "X2 Shape : (497, 18)\n",
      "y Shape : (497, 175)\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 18)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 18, 256)      44800       input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 4096)         0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 18, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          1048832     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 175)          44975       dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,729,711\n",
      "Trainable params: 1,729,711\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "16/16 - 1s - loss: 5.2147\n",
      "Epoch 2/50\n",
      "16/16 - 1s - loss: 4.3050\n",
      "Epoch 3/50\n",
      "16/16 - 1s - loss: 3.9226\n",
      "Epoch 4/50\n",
      "16/16 - 1s - loss: 3.5996\n",
      "Epoch 5/50\n",
      "16/16 - 1s - loss: 3.4641\n",
      "Epoch 6/50\n",
      "16/16 - 1s - loss: 3.3536\n",
      "Epoch 7/50\n",
      "16/16 - 1s - loss: 3.2732\n",
      "Epoch 8/50\n",
      "16/16 - 1s - loss: 3.0947\n",
      "Epoch 9/50\n",
      "16/16 - 1s - loss: 3.0378\n",
      "Epoch 10/50\n",
      "16/16 - 1s - loss: 2.8682\n",
      "Epoch 11/50\n",
      "16/16 - 1s - loss: 2.8169\n",
      "Epoch 12/50\n",
      "16/16 - 1s - loss: 2.7214\n",
      "Epoch 13/50\n",
      "16/16 - 1s - loss: 2.6012\n",
      "Epoch 14/50\n",
      "16/16 - 1s - loss: 2.5633\n",
      "Epoch 15/50\n",
      "16/16 - 1s - loss: 2.4302\n",
      "Epoch 16/50\n",
      "16/16 - 1s - loss: 2.3288\n",
      "Epoch 17/50\n",
      "16/16 - 1s - loss: 2.2916\n",
      "Epoch 18/50\n",
      "16/16 - 1s - loss: 2.1930\n",
      "Epoch 19/50\n",
      "16/16 - 1s - loss: 2.0871\n",
      "Epoch 20/50\n",
      "16/16 - 1s - loss: 2.0754\n",
      "Epoch 21/50\n",
      "16/16 - 1s - loss: 1.9795\n",
      "Epoch 22/50\n",
      "16/16 - 1s - loss: 1.8380\n",
      "Epoch 23/50\n",
      "16/16 - 1s - loss: 1.7442\n",
      "Epoch 24/50\n",
      "16/16 - 1s - loss: 1.6542\n",
      "Epoch 25/50\n",
      "16/16 - 1s - loss: 1.6178\n",
      "Epoch 26/50\n",
      "16/16 - 1s - loss: 1.5090\n",
      "Epoch 27/50\n",
      "16/16 - 1s - loss: 1.4144\n",
      "Epoch 28/50\n",
      "16/16 - 1s - loss: 1.3269\n",
      "Epoch 29/50\n",
      "16/16 - 1s - loss: 1.2424\n",
      "Epoch 30/50\n",
      "16/16 - 1s - loss: 1.1688\n",
      "Epoch 31/50\n",
      "16/16 - 1s - loss: 1.1033\n",
      "Epoch 32/50\n",
      "16/16 - 1s - loss: 1.0391\n",
      "Epoch 33/50\n",
      "16/16 - 1s - loss: 0.9466\n",
      "Epoch 34/50\n",
      "16/16 - 1s - loss: 0.8886\n",
      "Epoch 35/50\n",
      "16/16 - 1s - loss: 0.8395\n",
      "Epoch 36/50\n",
      "16/16 - 1s - loss: 0.7894\n",
      "Epoch 37/50\n",
      "16/16 - 1s - loss: 0.7023\n",
      "Epoch 38/50\n",
      "16/16 - 1s - loss: 0.6707\n",
      "Epoch 39/50\n",
      "16/16 - 1s - loss: 0.6342\n",
      "Epoch 40/50\n",
      "16/16 - 1s - loss: 0.6230\n",
      "Epoch 41/50\n",
      "16/16 - 1s - loss: 0.5622\n",
      "Epoch 42/50\n",
      "16/16 - 1s - loss: 0.5531\n",
      "Epoch 43/50\n",
      "16/16 - 1s - loss: 0.5276\n",
      "Epoch 44/50\n",
      "16/16 - 1s - loss: 0.4846\n",
      "Epoch 45/50\n",
      "16/16 - 1s - loss: 0.4479\n",
      "Epoch 46/50\n",
      "16/16 - 1s - loss: 0.4600\n",
      "Epoch 47/50\n",
      "16/16 - 1s - loss: 0.4488\n",
      "Epoch 48/50\n",
      "16/16 - 1s - loss: 0.4103\n",
      "Epoch 49/50\n",
      "16/16 - 1s - loss: 0.3887\n",
      "Epoch 50/50\n",
      "16/16 - 1s - loss: 0.3649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f740a20dbe0>"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)\n",
    "print(\"X1 Shape : {0}\".format(X1train.shape))\n",
    "print(\"X2 Shape : {0}\".format(X2train.shape))\n",
    "print(\"y Shape : {0}\".format(ytrain.shape))\n",
    "\n",
    "# define checkpoint callback\n",
    "#filepath = 'Image_captioning_model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "model = define_model(vocab_size, max_length)\n",
    "model.fit([X1train, X2train], ytrain, epochs=50, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5FodORu-s2gy"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "0a-CkW-w6zXY",
    "outputId": "64dbe6d1-0ce1-4c13-b959-3081e0953402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startseq two people and two horses watching endseq\n"
     ]
    }
   ],
   "source": [
    "# extract features from each photo in the directory\n",
    "def extract_features(filename):\n",
    "  # load the model\n",
    "  model = VGG16()\n",
    "  # re-structure the model\n",
    "  model.layers.pop()\n",
    "  model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "  # load the photo\n",
    "  image = load_img(filename, target_size=(224, 224))\n",
    "  # convert the image pixels to a numpy array\n",
    "  image = img_to_array(image)\n",
    "  # reshape data for the model\n",
    "  image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "  # prepare the image for the VGG model\n",
    "  image = preprocess_input(image)\n",
    "  # get features\n",
    "  feature = model.predict(image, verbose=0)\n",
    "  return feature\n",
    "\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "\n",
    "# load and prepare the photograph\n",
    "photo = extract_features(\"drive/My Drive/IC/testing.jpg\")\n",
    "\n",
    "# generate description\n",
    "description = generate_desc(model, tokenizer, photo, max_length)\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LPcf2yAzDvjF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Image_Captioning_on_Flicker8k.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
